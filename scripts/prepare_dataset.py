#!/usr/bin/env python3
"""
Dataset Preparation Script

Prepares the pothole image dataset for YOLO training by:
1. Splitting images into train/val sets
2. Creating synthetic labels (for demo) or processing existing annotations
3. Creating YOLO dataset configuration YAML

Usage:
    python scripts/prepare_dataset.py
    python scripts/prepare_dataset.py --val-split 0.2
"""

import os
import sys
import shutil
import random
import argparse
from pathlib import Path
from typing import List, Tuple

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def find_images(directory: Path, extensions: List[str] = None) -> List[Path]:
    """Find all image files in directory."""
    if extensions is None:
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    
    images = []
    for ext in extensions:
        images.extend(directory.glob(f"*{ext}"))
        images.extend(directory.glob(f"*{ext.upper()}"))
    
    return sorted(images)


def create_synthetic_labels(
    images: List[Path],
    output_dir: Path,
    class_id: int = 0
):
    """
    Create synthetic YOLO labels for demo purposes.
    
    In production, you would use actual annotations.
    This creates center-crop pseudo-labels for training demo.
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for img_path in images:
        # Create a synthetic label (center bounding box)
        # Format: class_id center_x center_y width height (normalized)
        
        # Random variation for demo
        cx = 0.5 + random.uniform(-0.15, 0.15)
        cy = 0.55 + random.uniform(-0.15, 0.1)  # Slightly below center
        w = 0.3 + random.uniform(-0.1, 0.2)
        h = 0.2 + random.uniform(-0.05, 0.15)
        
        # Clamp values
        cx = max(w/2, min(1-w/2, cx))
        cy = max(h/2, min(1-h/2, cy))
        
        label_path = output_dir / f"{img_path.stem}.txt"
        with open(label_path, 'w') as f:
            f.write(f"{class_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\n")
    
    return len(images)


def split_dataset(
    images: List[Path],
    val_split: float = 0.2,
    seed: int = 42
) -> Tuple[List[Path], List[Path]]:
    """Split images into train and validation sets."""
    random.seed(seed)
    images = images.copy()
    random.shuffle(images)
    
    n_val = int(len(images) * val_split)
    val_images = images[:n_val]
    train_images = images[n_val:]
    
    return train_images, val_images


def copy_files(
    images: List[Path],
    labels_dir: Path,
    output_images_dir: Path,
    output_labels_dir: Path
):
    """Copy images and labels to output directories."""
    output_images_dir.mkdir(parents=True, exist_ok=True)
    output_labels_dir.mkdir(parents=True, exist_ok=True)
    
    count = 0
    for img_path in images:
        # Copy image
        dst_img = output_images_dir / img_path.name
        shutil.copy(img_path, dst_img)
        
        # Copy label
        label_path = labels_dir / f"{img_path.stem}.txt"
        if label_path.exists():
            dst_label = output_labels_dir / label_path.name
            shutil.copy(label_path, dst_label)
            count += 1
    
    return count


def create_yaml_config(
    output_path: Path,
    train_images: Path,
    val_images: Path,
    class_names: List[str]
):
    """Create YOLO dataset YAML configuration."""
    content = f"""# Pothole Detection Dataset
# Auto-generated by prepare_dataset.py

path: {output_path.parent.absolute()}
train: {train_images.relative_to(output_path.parent)}
val: {val_images.relative_to(output_path.parent)}

# Classes
nc: {len(class_names)}
names: {class_names}
"""
    
    with open(output_path, 'w') as f:
        f.write(content)


def main():
    parser = argparse.ArgumentParser(
        description="Prepare pothole dataset for YOLO training"
    )
    parser.add_argument(
        '--val-split', type=float, default=0.2,
        help='Validation split ratio (default: 0.2)'
    )
    parser.add_argument(
        '--source', type=str, default='Datasets/Pothole_Image_Data',
        help='Source image directory'
    )
    parser.add_argument(
        '--output', type=str, default='Datasets',
        help='Output directory for prepared dataset'
    )
    parser.add_argument(
        '--seed', type=int, default=42,
        help='Random seed for splitting'
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸ“¦ POTHOLE DATASET PREPARATION")
    print("="*60)
    
    # Paths
    source_dir = project_root / args.source
    output_dir = project_root / args.output
    
    # Check source exists
    if not source_dir.exists():
        print(f"\nâŒ Source directory not found: {source_dir}")
        return 1
    
    print(f"\nğŸ“‚ Source: {source_dir}")
    print(f"ğŸ“‚ Output: {output_dir}")
    print(f"ğŸ“Š Validation split: {args.val_split:.0%}")
    
    # Find images
    print("\nğŸ” Finding images...")
    all_images = find_images(source_dir)
    print(f"   Found {len(all_images)} images")
    
    if len(all_images) == 0:
        print("âŒ No images found!")
        return 1
    
    # Create labels directory
    labels_dir = output_dir / "labels_temp"
    
    # Create synthetic labels (for demo)
    print("\nğŸ“ Creating labels...")
    n_labels = create_synthetic_labels(all_images, labels_dir)
    print(f"   Created {n_labels} label files")
    
    # Split dataset
    print("\nâœ‚ï¸  Splitting dataset...")
    train_images, val_images = split_dataset(all_images, args.val_split, args.seed)
    print(f"   Train: {len(train_images)} images")
    print(f"   Val: {len(val_images)} images")
    
    # Create output structure
    train_img_dir = output_dir / "train/images"
    train_lbl_dir = output_dir / "train/labels"
    val_img_dir = output_dir / "val/images"
    val_lbl_dir = output_dir / "val/labels"
    
    # Copy files
    print("\nğŸ“‹ Copying files...")
    n_train = copy_files(train_images, labels_dir, train_img_dir, train_lbl_dir)
    n_val = copy_files(val_images, labels_dir, val_img_dir, val_lbl_dir)
    print(f"   Train: {n_train} image-label pairs")
    print(f"   Val: {n_val} image-label pairs")
    
    # Create YAML config
    print("\nâš™ï¸  Creating dataset configuration...")
    yaml_path = output_dir / "pothole_dataset.yaml"
    create_yaml_config(
        yaml_path,
        train_img_dir,
        val_img_dir,
        class_names=['pothole']
    )
    print(f"   Created: {yaml_path}")
    
    # Cleanup temp labels
    shutil.rmtree(labels_dir)
    
    # Summary
    print("\n" + "="*60)
    print("âœ… DATASET PREPARED!")
    print("="*60)
    print(f"\nğŸ“Š Dataset structure:")
    print(f"   {output_dir}/")
    print(f"   â”œâ”€â”€ train/")
    print(f"   â”‚   â”œâ”€â”€ images/ ({len(train_images)} files)")
    print(f"   â”‚   â””â”€â”€ labels/ ({len(train_images)} files)")
    print(f"   â”œâ”€â”€ val/")
    print(f"   â”‚   â”œâ”€â”€ images/ ({len(val_images)} files)")
    print(f"   â”‚   â””â”€â”€ labels/ ({len(val_images)} files)")
    print(f"   â””â”€â”€ pothole_dataset.yaml")
    
    print("\nğŸ’¡ Next steps:")
    print("   1. Review/update labels if you have real annotations")
    print("   2. Run training: python scripts/train.py")
    
    return 0


if __name__ == "__main__":
    sys.exit(main() or 0)
